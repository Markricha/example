{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb5d372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:05:42.448166Z",
     "iopub.status.busy": "2024-02-11T01:05:42.447659Z",
     "iopub.status.idle": "2024-02-11T01:06:02.159560Z",
     "shell.execute_reply": "2024-02-11T01:06:02.158477Z"
    },
    "papermill": {
     "duration": 19.723118,
     "end_time": "2024-02-11T01:06:02.162029",
     "exception": false,
     "start_time": "2024-02-11T01:05:42.438911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:(1502, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "#import h5py\n",
    "from urllib.parse import urlencode  # For Python 3\n",
    "\n",
    "# Check if the log file already exists; if not, create it\n",
    "log_file_path = 'catcher.log'\n",
    "if not os.path.exists(log_file_path):\n",
    "    open(log_file_path, 'a').close()  # Create an empty log file\n",
    "\n",
    "# Configure logging to write to both console and file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Log to console\n",
    "        logging.FileHandler(log_file_path)  # Log to file\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the 'data2' directory if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# Configure logging to write to both console and file\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# ... (logging handlers configuration remains the same)\n",
    "\n",
    "url = 'http://aws.okx.com'\n",
    "history_candles_base = '/api/v5/market/history-candles?instId=BTC-USDT-SWAP&bar=1Dutc'\n",
    "\n",
    "store = pd.HDFStore('data/catcher.h5', 'w')\n",
    "\n",
    "try:\n",
    "    df = store['a0']\n",
    "except KeyError:  # If 'data2' key does not exist in HDF5 filedf = df.sort_values(by='ts')\n",
    "    df = pd.DataFrame()\n",
    "    pass\n",
    "try:\n",
    "    df.rename(columns={'open': 'Open','high': 'High', 'low':'Low', 'close':'Close','volume': 'Volume'}, inplace=True)\n",
    "except KeyError:\n",
    "    print(\"One or more columns specified for renaming were not found in the DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Continue with your code assuming df is now either loaded or initialized as empty\n",
    "\n",
    "after_value = df.iloc[-1, 0] if not df.empty else None\n",
    "before_value = df.iloc[0, 0] if not df.empty else None\n",
    "\n",
    "# Convert columns to numeric or string\n",
    "#for col in ['ts', 'Open', 'High', 'Low', 'Close','Volume', 'volCcy', 'volCcyQuote','confirm']:\n",
    "#    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(str)\n",
    "        \n",
    "def fetch_and_append_data(after=None):\n",
    "    logger.info('ftch_and_append_data()')\n",
    "    params = {}\n",
    "    if after:\n",
    "        params['after'] = after\n",
    "\n",
    "    history_candles_url = f\"{url}{history_candles_base}&{urlencode(params)}\"\n",
    "\n",
    "    logger.info(f\"Sending request to {history_candles_url}\")\n",
    "\n",
    "    response = requests.get(history_candles_url)\n",
    "    response_json = json.loads(response.text)\n",
    "\n",
    "    # Assuming 'data' key contains the actual candlestick data with the expected column names\n",
    "    new_df = pd.DataFrame(response_json['data'], columns=['ts', 'Open', 'High', 'Low', 'Close','Volume', 'volCcy', 'volCcyQuote', 'confirm'])\n",
    "    \n",
    "    if new_df.empty:\n",
    "        return\n",
    "    \n",
    "    # Make sure the 'ts' column is converted to numeric if possible, else convert it to string\n",
    "    new_df['ts'] = pd.to_numeric(new_df['ts'], errors='coerce')\n",
    "\n",
    "    global df\n",
    "    \n",
    "    # Check for next page (pagination) based on the length of the new data batch\n",
    "    if len(new_df) >= 100:  # Assuming each batch returns at least 100 rows\n",
    "        df = pd.concat([df, new_df], axis=0).reset_index(drop=True)  # Append new_df to df first\n",
    "\n",
    "        # Save DataFrame to HDF5\n",
    "        logger.info(\"Saving DataFrame to HDF5 file...\")\n",
    "        store.put('a0', df, format='table')\n",
    "        logger.info(\"DataFrame saved successfully.\")\n",
    "\n",
    "        after_value = new_df.iloc[-1, 0]  # Get the timestamp of the last row in the new batch\n",
    "        fetch_and_append_data(after=after_value)\n",
    "\n",
    "    else:  # If the new batch has less than 100 rows (assuming this indicates the end of pagination)\n",
    "        df = pd.concat([df, new_df], axis=0).reset_index(drop=True)  # Append new_df to df even if it's the last batch\n",
    "\n",
    "        # Convert columns to numeric or string\n",
    "        for col in ['ts', 'Open', 'High', 'Low', 'Close', 'Volume', 'volCcy', 'volCcyQuote', 'confirm']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # Save DataFrame to HDF5\n",
    "        logger.info(\"Saving DataFrame to HDF5 file...\")\n",
    "        store.put('a0', df, format='table')\n",
    "        logger.info(\"DataFrame saved successfully.\")\n",
    "\n",
    "        # If there are no more pages or the last page had less than 100 rows, save the final DataFrame\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        # Jump out of the function\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "def update(before=None):\n",
    "    logger.info('update()')\n",
    "    params = {}\n",
    "    if before:\n",
    "        params['before'] = before\n",
    "\n",
    "    history_candles_url = f\"{url}{history_candles_base}&{urlencode(params)}\"\n",
    "\n",
    "    logger.info(f\"Sending request to {history_candles_url}\")\n",
    "\n",
    "    response = requests.get(history_candles_url)\n",
    "    response_json = json.loads(response.text)\n",
    "\n",
    "    # Assuming 'data' key contains the actual candlestick data with the expected column names\n",
    "    new_df = pd.DataFrame(response_json['data'], columns=['ts', 'Open', 'High', 'Low', 'Close','Volume', 'volCcy', 'volCcyQuote','confirm'])\n",
    "    \n",
    "    # Make sure the 'ts' column is converted to numeric if possible, else convert it to string\n",
    "    new_df['ts'] = pd.to_numeric(new_df['ts'], errors='coerce')\n",
    "    \n",
    "    # If there's no new data, stop fetching\n",
    "    if new_df.empty:\n",
    "        return\n",
    "    \n",
    "    global df\n",
    "    \n",
    "    # Check for next page (pagination) based on the length of the new data batch\n",
    "    if len(new_df) >= 100:  # Assuming each batch returns at least 100 rows\n",
    "        df = pd.concat([new_df, df], axis=0).reset_index(drop=True)  # Append new_df to df first\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        # Save DataFrame to HDF5\n",
    "        logger.info(\"Saving DataFrame to HDF5 file...\")\n",
    "        store.put('a0', df, format='table')\n",
    "        logger.info(\"DataFrame saved successfully.\")\n",
    "\n",
    "        before_value = new_df.iloc[0, 0]  # Get the timestamp of the last row in the new batch\n",
    "        update(before=before_value)\n",
    "\n",
    "    else:  # If the new batch has less than 100 rows (assuming this indicates the end of pagination)\n",
    "        df = pd.concat([new_df, df], axis=0).reset_index(drop=True)  # Append new_df to df even if it's the last batch\n",
    "\n",
    "        # Convert columns to numeric or string\n",
    "        for col in ['ts', 'Open', 'High', 'Low', 'Close', 'Volume', 'volCcy', 'volCcyQuote', 'confirm']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # Save DataFrame to HDF5\n",
    "        logger.info(\"Saving DataFrame to HDF5 file...\")\n",
    "        store.put('a0', df, format='table')\n",
    "        logger.info(\"DataFrame saved successfully.\")\n",
    "\n",
    "        # If there are no more pages or the last page had less than 100 rows, save the final DataFrame\n",
    "\n",
    "        # Jump out of the function\n",
    "        return df\n",
    "\n",
    "fetch_and_append_data(after_value)\n",
    "\n",
    "before_value = df.iloc[0, 0] if not df.empty else None\n",
    "update(before_value) \n",
    "\n",
    "store.close()\n",
    "\n",
    "# Assuming your DataFrame is loaded with 'ts' column in milliseconds\n",
    "df['datetime'] = pd.to_datetime(df['ts'], unit='ms')  # Convert timestamps to datetime (ignore the FutureWarning for now)\n",
    "df.set_index('datetime', inplace=True)  # Set datetime as the index for time series operations\n",
    "df['weekday'] = df.index.weekday  # Weekday: Monday=0, Tuesday=1, ..., Sunday=6\n",
    "# Optionally add other time-related features:\n",
    "df['day'] = df.index.day\n",
    "df = df.drop(columns = ['confirm'])\n",
    "# Instead of reopening an existing store, create a new one with the desired filename\n",
    "new_store_filename = './data//catcher.h5'\n",
    "store = pd.HDFStore(new_store_filename)\n",
    "\n",
    "logging.info(\"Saving DataFrame with preload to new HDF5 file...\")\n",
    "store.put('a1', df, format='table')\n",
    "\n",
    "# Don't forget to close the store after saving\n",
    "store.close()\n",
    "\n",
    "# Logging that the script has finished execution\n",
    "logger.info(f\"Script execution completed. DataFrame shape: {df.shape}\")\n",
    "print(f'df:{df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e868e813",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:02.175760Z",
     "iopub.status.busy": "2024-02-11T01:06:02.175252Z",
     "iopub.status.idle": "2024-02-11T01:06:02.185165Z",
     "shell.execute_reply": "2024-02-11T01:06:02.183882Z"
    },
    "papermill": {
     "duration": 0.018877,
     "end_time": "2024-02-11T01:06:02.187451",
     "exception": false,
     "start_time": "2024-02-11T01:06:02.168574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0 1500    2 1501    1 1496    6    3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "signal = df['Close']\n",
    "\n",
    "# Assuming signal is your input data\n",
    "fft_result = np.abs(np.fft.fft(signal))\n",
    "\n",
    "# Get the indices of the three largest values\n",
    "top_3= fft_result.argsort()[-8:][::-1]\n",
    "slow_Upper = min(top_3[1],top_3[2])\n",
    "length_Upper = min(top_3[3],top_3[4])\n",
    "fast_Upper = min(top_3[5],top_3[6])\n",
    "print(top_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d60063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:02.200254Z",
     "iopub.status.busy": "2024-02-11T01:06:02.199922Z",
     "iopub.status.idle": "2024-02-11T01:06:21.865558Z",
     "shell.execute_reply": "2024-02-11T01:06:21.864316Z"
    },
    "papermill": {
     "duration": 19.674834,
     "end_time": "2024-02-11T01:06:21.868090",
     "exception": false,
     "start_time": "2024-02-11T01:06:02.193256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas_ta\r\n",
      "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from pandas_ta) (2.1.4)\r\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas->pandas_ta) (1.24.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->pandas_ta) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->pandas_ta) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->pandas_ta) (2023.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.16.0)\r\n",
      "Building wheels for collected packages: pandas_ta\r\n",
      "  Building wheel for pandas_ta (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218910 sha256=ffa92a421ee3531e5330b5130255e09fa0a4efa0489a15c06f27c5ded494ab95\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/69/00/ac/f7fa862c34b0e2ef320175100c233377b4c558944f12474cf0\r\n",
      "Successfully built pandas_ta\r\n",
      "Installing collected packages: pandas_ta\r\n",
      "Successfully installed pandas_ta-0.3.14b0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2eb7b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:21.884384Z",
     "iopub.status.busy": "2024-02-11T01:06:21.883976Z",
     "iopub.status.idle": "2024-02-11T01:06:26.320845Z",
     "shell.execute_reply": "2024-02-11T01:06:26.319833Z"
    },
    "papermill": {
     "duration": 4.448064,
     "end_time": "2024-02-11T01:06:26.323080",
     "exception": false,
     "start_time": "2024-02-11T01:06:21.875016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1502, 415)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import h5py\n",
    "import warnings\n",
    "from sklearn.utils import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import gc\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = df.sort_values(by='ts')\n",
    "\n",
    "# Assuming n_clusters is the number of clusters you want\n",
    "n_clusters = 3\n",
    "\n",
    "# Initialize the KMeans model\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "# target drop\n",
    "df['N_C'] = df['Close'].shift(-1)\n",
    "df['N_H'] = df['High'].shift(-1)\n",
    "df['N_L'] = df['Low'].shift(-1)\n",
    "df['N_V'] = df['Volume'].shift(-1)\n",
    "\n",
    "\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "# define target  \n",
    "df['tarPer_C'] = (df['N_C'] - df['Close'])/ df['Close']\n",
    "df['tarPer_H'] = (df['N_H'] - df['Close']) / df['Close']\n",
    "df['tarPer_L']= (df['N_L'] - df['Close']) / df['Close']\n",
    "df['tarPer_V']= (df['N_V'] - df['Volume']) / df['Volume']\n",
    "\n",
    "# drop furtue index\n",
    "df = df.drop(columns=['N_C', 'N_H', 'N_L', 'N_V'])\n",
    "\n",
    "\n",
    "df['High'] = pd.to_numeric(df['High'], errors='coerce')\n",
    "df['Low'] = pd.to_numeric(df['Low'], errors='coerce')\n",
    "# High-Low midpoint (HL2)\n",
    "df['HL2']=(df['High']+df['Low']) / 2\n",
    "\n",
    "\n",
    "# Typical Price (HLC3)\n",
    "df.ta.hlc3(offset=None)\n",
    "\n",
    "# Volume Weighted Average Price (VWAP)\n",
    "df.ta.vwap(anchor=None, offset=None, append=True)\n",
    "\n",
    "\n",
    "# Weighted Close Price (WCP)\n",
    "df.ta.wcp(offset=None, append=True)\n",
    "\n",
    "\n",
    "df['ML'] = df['High'] - df[\"Low\"]\n",
    "df['PV_ML'] = df['ML'] / df['Volume']\n",
    "df['BDL'] = df['Close'] - df['Low']\n",
    "df['AKL'] = df['High'] - df['Close']\n",
    "df['PV_BDL'] = df['BDL'] / df['Volume']\n",
    "df['PV_AKL'] = df['AKL'] / df['Volume']\n",
    "df['bull_l'] = df['BDL'] - df['AKL']\n",
    "df['divbull_l'] = df['BDL'] / df['AKL']\n",
    "\n",
    "\n",
    "df['PS_AKL'] = df['AKL'] / df['ML']\n",
    "\n",
    "\n",
    "df['bullrock'] = df['Close'] - df['Open']\n",
    "df['SA'] = df['ML'] - df['bullrock']\n",
    "df['max_bull_zone'] = df[['Close', 'Open']].sub(df['Low'], axis=0).max(axis=1)\n",
    "df['up_SA'] = df['ML'] - df['max_bull_zone']\n",
    "df['low_SA'] = df['max_bull_zone'] - abs(df['bullrock'])\n",
    "df['subUL'] = df['up_SA'] - df['low_SA']\n",
    "df['divUL'] = df['up_SA'] / df['low_SA']\n",
    "df['posLM'] = df['max_bull_zone'] / df['ML']\n",
    "df['pos_low_SA'] = df['low_SA']/ df['ML']\n",
    "df['pos_bullrock'] = df['bullrock']/ +df['ML']\n",
    "df['PVLM'] = df['max_bull_zone'] / df['Volume']\n",
    "df['PV_up_SA'] = df['up_SA'] / (  df['Volume'])\n",
    "df['PV_low_SA'] = df['low_SA']/ (  df['Volume'])\n",
    "df['PV_bullrock'] = df['bullrock']/ (  df['Volume'])\n",
    "df['PV_SA'] = df['SA']/ (  df['Volume'])\n",
    "\n",
    "\n",
    "len_c = len(df['Close'])\n",
    "df['C_r'] = df['Close'].rank(ascending=False) / len_c\n",
    "df['H_r'] = df['High'].rank(ascending=False) / len_c\n",
    "df['L_r'] = df['Low'].rank(ascending=False) / len_c\n",
    "df['V_r'] = df['Volume'].rank(ascending=False) / len_c\n",
    "\n",
    "df['W_r'] = df['VWAP_D'].rank(ascending=False) / len_c\n",
    "\n",
    "    \n",
    "values, counts = np.unique(df['Volume'], return_counts=True)\n",
    "weights = compute_class_weight('balanced', classes=values, y=df['Volume'])\n",
    "df['Vgini'] = 1 - np.sum((weights * counts) ** 2) / (np.sum(counts) ** 2)\n",
    "\n",
    "values, counts = np.unique(df['VWAP_D'], return_counts=True)\n",
    "weights = compute_class_weight('balanced', classes=values, y=df['VWAP_D'])\n",
    "df['Wgini'] = 1 - np.sum((weights * counts) ** 2) / (np.sum(counts) ** 2)\n",
    "\n",
    "values, counts = np.unique(df['Close'], return_counts=True)\n",
    "weights = compute_class_weight('balanced', classes=values, y=df['Close'])\n",
    "df['Cgini'] = 1 - np.sum((weights * counts) ** 2) / (np.sum(counts) ** 2)   \n",
    "\n",
    "bounds_list = [length_Upper, slow_Upper, fast_Upper]\n",
    "\n",
    "for length_ in bounds_list:\n",
    "\n",
    "    df['L_C'+str(length_)] = df['Close'].shift(length_)\n",
    "    df['L_H'+str(length_)] = df['High'].shift(length_)\n",
    "    df['L_L'+str(length_)] = df['Low'].shift(length_)\n",
    "    df['L_V'+str(length_)] = df['Volume'].shift(length_)\n",
    "    df['L_O'+str(length_)] = df['Open'].shift(length_)\n",
    "\n",
    "    df['L_C'+str(length_)] = df['L_C'+str(length_)].fillna(0)\n",
    "    df['L_O'+str(length_)] = df['L_O'+str(length_)].fillna(0)\n",
    "    df['L_L'+str(length_)] = df['L_L'+str(length_)].fillna(0)    \n",
    "\n",
    "    df['amihud'+str(length_)] = (2*(df['L_H'+str(length_)] - df['L_L'+str(length_)]) - abs(df['L_O'+str(length_)] - df['L_C'+str(length_)])) / df['L_V'+str(length_)]\n",
    "\n",
    "    df['ADClose'+str(length_)] = df['Close'] - df['L_C'+str(length_)]\n",
    "    df['ADHigh'+str(length_)] = df['High'] - df['L_H'+str(length_)]\n",
    "    df['ADLow'+str(length_)] = df['Low'] - df['L_L'+str(length_)]\n",
    "    df['ADVolume'+str(length_)] = df['Volume'] - df['L_V'+str(length_)]\n",
    "\n",
    "    df['PV_ADClose'+str(length_)] = df['ADClose'+str(length_)] / (  df['L_V'+str(length_)] )\n",
    "    df['PV_ADHigh'+str(length_)] = df['ADHigh'+str(length_)] / (  df['L_V'+str(length_)] )\n",
    "    df['PV_Low'+str(length_)] = df['ADLow'+str(length_)] / (  df['L_V'+str(length_)] )\n",
    "\n",
    "    df['divClose'+str(length_)] = df['Close'] / (  df['L_C'+str(length_)])\n",
    "    df['divHigh'+str(length_)] = df['High'] / (  df['L_H'+str(length_)])\n",
    "    df['divLow'+str(length_)] = df['Low'] / (  df['L_L'+str(length_)])\n",
    "    df['divVolume'+str(length_)] = df['Volume'] / (  df['L_V'+str(length_)]    )\n",
    "\n",
    "    df['DELow'+str(length_)] = df['ADLow'+str(length_)] / (  df['L_L'+str(length_)])\n",
    "    df['ML'+str(length_)]  = df['L_H'+str(length_)] - df['L_L'+str(length_)] \n",
    "    df['PV_ML' +str(length_)]= df['ML'+str(length_)]  / (  df['L_V'+str(length_)] )\n",
    "    df['BDL'+str(length_)] = df['L_C'+str(length_)]- df['L_L'+str(length_)] \n",
    "    df['AKL'+str(length_)] = df['L_H'+str(length_)]- df['L_C'+str(length_)]\n",
    "    df['PV_BDL'+str(length_)]= df['BDL'+str(length_)] / (  df['L_V'+str(length_)] )\n",
    "    df['PV_AKL'+str(length_)] = df['AKL'+str(length_)] / (  df['L_V'+str(length_)] )\n",
    "    df['bull_l'+str(length_)] = df['BDL'+str(length_)] - df['AKL'+str(length_)]\n",
    "    df['divbull_l'+str(length_)] = df['BDL'+str(length_)] / (  df['AKL'+str(length_)])\n",
    "    df['PS_AKL'+str(length_)] = df['AKL'+str(length_)] / (  df['ML'+str(length_)] )\n",
    "    df['bullrock'+str(length_)] = df['L_C'+str(length_)]- df['L_O'+str(length_)]\n",
    "    df['SA'+str(length_)] = df['ML'+str(length_)]  - df['bullrock'+str(length_)]\n",
    "\n",
    "    df['max_bull_zone'+str(length_)] = np.where(df['bullrock'+str(length_)] >= 0,\n",
    "                                          df['L_C'+str(length_)] - df['L_L'+str(length_)],\n",
    "                                          df['L_O'+str(length_)] - df['L_L'+str(length_)])\n",
    "\n",
    "    df['up_SA'+str(length_)] = df['ML'+str(length_)]  - df['max_bull_zone'+str(length_)]\n",
    "    df['low_SA'+str(length_)] = df['max_bull_zone'+str(length_)] - abs(df['bullrock'+str(length_)])\n",
    "\n",
    "    df['subUL' ] = df['up_SA'+ str(length_) ] - df['low_SA'+ str(length_) ]\n",
    "    df['divUL'+ str(length_) ] = df['up_SA'+ str(length_) ] / (  df['low_SA'+ str(length_) ])\n",
    "\n",
    "    df['posLM'+str(length_)] = df['max_bull_zone'+str(length_)] / (  df['ML'+str(length_)] )\n",
    "    df['pos_up_SA'+str(length_)] = df['up_SA'+str(length_)] / (  df['ML'+str(length_)] )\n",
    "    df['pos_low_SA'+str(length_)] = df['low_SA'+str(length_)]/ (  df['ML'+str(length_)] )\n",
    "    df['pos_bullrock'+str(length_)] = df['bullrock'+str(length_)]/ (  df['ML'+str(length_)] )\n",
    "    df['pos_SA'+str(length_)] = df['SA'+str(length_)]/ (  df['ML'+str(length_)] )\n",
    "\n",
    "    df['PVLM'+str(length_)] = df['max_bull_zone'+str(length_)] / (  df['L_V'+str(length_)] )\n",
    "    df['PV_up_SA'+str(length_)] = df['up_SA'+str(length_)] / (  df['L_V'+str(length_)] )\n",
    "    df['PV_low_SA'+str(length_)] = df['low_SA'+str(length_)]/ (  df['L_V'+str(length_)] )\n",
    "    df['PV_bullrock'+str(length_)] = df['bullrock'+str(length_)]/ (  df['L_V'+str(length_)] )\n",
    "    df['PV_SA'+str(length_)] = df['SA'+str(length_)]/ (  df['L_V'+str(length_)] )\n",
    "\n",
    "    # sub\n",
    "\n",
    "    df['sub0ML'+str(length_)]  = df['ML'] - df['ML'+str(length_)] \n",
    "    df['sub0PV_ML' +str(length_)]=  df['PV_ML'] - df['PV_ML' +str(length_)]\n",
    "    df['sub0BDL'+str(length_)] = df['BDL'] - df['BDL'+str(length_)]  \n",
    "    df['sub0AKL'+str(length_)] = df['AKL'] - df['AKL'+str(length_)] \n",
    "    df['sub0PV_BDL'+str(length_)] = df['PV_BDL'] - df['PV_BDL'+str(length_)] \n",
    "    df['sub0PV_AKL'+str(length_)] = df['PV_AKL'] - df['PV_AKL'+str(length_)] \n",
    "    df['sub0bull_l'+str(length_)] = df['bull_l'] - df['bull_l'+str(length_)] \n",
    "    df['sub0divbull_l'+str(length_)] = df['divbull_l'] - df['divbull_l'+str(length_)] \n",
    "\n",
    "    #df['sub0logbull_l'+str(length_)] = df['logbull_l'] - df['logbull_l'+str(length_)] \n",
    "    #df['sub0PS_BDL'+str(length_)] = df['PS_BDL'] - df['PS_BDL'+str(length_)] \n",
    "\n",
    "    df['sub0bullrock'+str(length_)] = df['bullrock'] - df['bullrock'+str(length_)] \n",
    "    df['sub0SA'+str(length_)] = df['SA'] - df['SA'+str(length_)]  \n",
    "    df['sub_fuLM'+str(length_)] = df['max_bull_zone'] - df['max_bull_zone'+str(length_)] \n",
    "    df['sub0up_SA'+str(length_)] = df['up_SA'] - df['up_SA'+str(length_)] \n",
    "    df['sub0low_SA'+str(length_)] = df['low_SA'] - df['low_SA'+str(length_)] \n",
    "\n",
    "    df['sub0posLM'+str(length_)] = df['posLM'] - df['posLM'+str(length_)] \n",
    "    #df['sub0pos_up_SA'+str(length_)] = df['pos_up_SA'] - df['pos_up_SA'+str(length_)] \n",
    "    df['sub0pos_low_SA'+str(length_)] = df['pos_low_SA'] - df['pos_low_SA'+str(length_)] \n",
    "    df['sub0pos_bullrock'+str(length_)] = df['pos_bullrock'] - df['pos_bullrock'+str(length_)] \n",
    "\n",
    "    df['sub0PVLM'+str(length_)] = df['PVLM'] - df['PVLM'+str(length_)] \n",
    "    df['sub0PV_up_SA'+str(length_)] = df['PV_up_SA'] - df['PV_up_SA'+str(length_)]  \n",
    "    df['sub0PV_low_SA'+str(length_)] = df['PV_low_SA'] - df['PV_low_SA'+str(length_)] \n",
    "    df['sub0PV_bullrock'+str(length_)] = df['PV_bullrock'] - df['PV_bullrock'+str(length_)] \n",
    "    df['sub0PV_SA'+str(length_)] = df['PV_SA'] - df['PV_SA'+str(length_)] \n",
    "\n",
    "    # div\n",
    "\n",
    "    df['div0ML'+str(length_)]  = df['ML'] / (  df['ML'+str(length_)] )\n",
    "    df['div0PV_ML' +str(length_)]=  df['PV_ML'] / (  df['PV_ML' +str(length_)])\n",
    "    df['div0BDL'+str(length_)] = df['BDL'] / (  df['BDL'+str(length_)]  )\n",
    "    df['div0AKL'+str(length_)] = df['AKL'] / (  df['AKL'+str(length_)] )\n",
    "    df['div0PV_BDL'+str(length_)] = df['PV_BDL'] / (  df['PV_BDL'+str(length_)] )\n",
    "    df['div0PV_AKL'+str(length_)] = df['PV_AKL'] / (  df['PV_AKL'+str(length_)] )\n",
    "    df['div0bull_l'+str(length_)] = df['bull_l'] / (  df['bull_l'+str(length_)] )\n",
    "\n",
    "    #df['div0logbull_l'+str(length_)] = df['logbull_l'] / (  df['logbull_l'+str(length_)] )\n",
    "    #df['div0PS_BDL'+str(length_)] = df['PS_BDL'] / (  df['PS_BDL'+str(length_)] )\n",
    "    df['div0PS_AKL'+str(length_)] = df['PS_AKL'] / (  df['PS_AKL'+str(length_)] )\n",
    "    #df['div0PS_bull_l'+str(length_)] = df['PS_bull_l'] / (  df['PS_bull_l'+str(length_)] )\n",
    "\n",
    "    df['div0bullrock'+str(length_)] = df['bullrock'] / (  df['bullrock'+str(length_)] )\n",
    "    df['div0SA'+str(length_)] = df['SA'] / (  df['SA'+str(length_)]  )\n",
    "    df['div_fuLM'+str(length_)] = df['max_bull_zone'] / (  df['max_bull_zone'+str(length_)] )\n",
    "    df['div0low_SA'+str(length_)] = df['low_SA'] / (  df['low_SA'+str(length_)] )\n",
    "\n",
    "    #df['div0pos_up_SA'+str(length_)] = df['pos_up_SA'] / (  df['pos_up_SA'+str(length_)] )\n",
    "    df['div0pos_low_SA'+str(length_)] = df['pos_low_SA'] / (  df['pos_low_SA'+str(length_)] )\n",
    "    df['div0pos_bullrock'+str(length_)] = df['pos_bullrock'] / (  df['pos_bullrock'+str(length_)] )\n",
    "    #df['div0pos_SA'+str(length_)] = df['pos_SA'] / (  df['pos_SA'+str(length_)]  )\n",
    "\n",
    "    df['div0PVLM'+str(length_)] = df['PVLM'] / (  df['PVLM'+str(length_)] )\n",
    "    df['div0PV_up_SA'+str(length_)] = df['PV_up_SA'] / (  df['PV_up_SA'+str(length_)]  )\n",
    "    df['div0PV_low_SA'+str(length_)] = df['PV_low_SA'] / (  df['PV_low_SA'+str(length_)] )\n",
    "    df['div0PV_bullrock'+str(length_)] = df['PV_bullrock'] / (  df['PV_bullrock'+str(length_)] )\n",
    "    df['div0PV_SA'+str(length_)] = df['PV_SA'] / (  df['PV_SA'+str(length_)] )\n",
    "\n",
    "    # log\n",
    "\n",
    "    df['log0PV_BDL'+str(length_)] = np.log(abs(df['PV_BDL'] ))- np.log(abs(df['PV_BDL'+str(length_)] ))\n",
    "    df['log0PV_AKL'+str(length_)] = np.log(abs(df['PV_AKL'])) - np.log(abs(df['PV_AKL'+str(length_)] ))\n",
    "    df['log0bull_l'+str(length_)] = np.log(abs(df['bull_l'])) - np.log(abs(df['bull_l'+str(length_)] ))\n",
    "    #df['log0logbull_l'+str(length_)] = np.log(abs(df['logbull_l'])) - np.log(abs(df['logbull_l'+str(length_)]) )\n",
    "    #df['log0PS_bull_l'+str(length_)] = np.log(abs(df['PS_bull_l']) )- np.log(abs(df['PS_bull_l'+str(length_)]) )\n",
    "    df['log0bullrock'+str(length_)] = np.log(abs(df['bullrock'])) - np.log(abs(df['bullrock'+str(length_)]) )\n",
    "    df['log0pos_bullrock'+str(length_)] = np.log(abs(df['pos_bullrock'])) - np.log(abs(df['pos_bullrock'+str(length_)]))\n",
    "    df['log0PV_bullrock'+str(length_)] = np.log(abs(df['PV_bullrock']) )- np.log(abs(df['PV_bullrock'+str(length_)]) )\n",
    "    df['log0PV_SA'+str(length_)] = np.log(abs(df['PV_SA'])) - np.log(abs(df['PV_SA'+str(length_)])) \n",
    "\n",
    "\n",
    "def convert_days_to_periods(d, x):\n",
    "    total_days = len(x)\n",
    "    return int(np.floor(total_days * d))\n",
    "\n",
    "def rank(x: pd.Series):\n",
    "    # Assuming pct=True gives percentage rank\n",
    "    return x.rank(pct=True)\n",
    "\n",
    "def delay(x: pd.Series, d: int):\n",
    "    if d < 0 or d >= len(x):\n",
    "        raise ValueError(f\"Invalid delay value d={d}. It should be between 0 and {len(x) - 1}\")\n",
    "    return x.shift(d)\n",
    "\n",
    "def correlation(x: pd.Series, y: pd.Series, d: float):\n",
    "    window_size = convert_days_to_periods(d, x.index)\n",
    "    return x.rolling(window=window_size).corr(y)\n",
    "\n",
    "def rolling_covariance(x: pd.Series, y: pd.Series, window_size: int):\n",
    "    return x.rolling(window=window_size).cov(y)\n",
    "\n",
    "def scale(x: pd.Series, a: float = 1.0):\n",
    "    norm = np.sum(np.abs(x))\n",
    "    return x * (a / norm)\n",
    "\n",
    "def delta(x: pd.Series, d: int):\n",
    "    return x - delay(x, d)\n",
    "\n",
    "def signedpower(x: pd.Series, a: float):\n",
    "    return np.sign(x) * np.abs(x) ** a\n",
    "\n",
    "def decay_linear(x: pd.Series, d: float):\n",
    "    weights = np.arange(1, convert_days_to_periods(d+1, x)+1)[::-1] / sum(np.arange(1, convert_days_to_periods(d+1, x)+1))\n",
    "    return x.rolling(window=len(weights), min_periods=1).apply(lambda w: np.dot(w, weights))\n",
    "\n",
    "def indneutralize(x: pd.Series, g: pd.Series):\n",
    "    group_means = x.groupby(g).transform('mean')\n",
    "    return x - group_means\n",
    "\n",
    "def ts_min(x: pd.Series, d: int):\n",
    "    return x.rolling(d).min()\n",
    "\n",
    "def ts_max(x: pd.Series, d: int):\n",
    "    return x.rolling(d).max()\n",
    "\n",
    "def ts_min_day(x: pd.Series, d: int):\n",
    "    # Get the index of the minimum values\n",
    "    min_values = x.rolling(d).min()\n",
    "    min_indices = min_values.idxmin()\n",
    "    \n",
    "    # Extract the day component from the DatetimeIndex\n",
    "    days = [x.index[i].day for i in min_indices]\n",
    "    return pd.Series(days, index=min_indices.index)\n",
    "\n",
    "def ts_max_day(x: pd.Series, d: int):\n",
    "    # Same process for the maximum values\n",
    "    max_values = x.rolling(d).max()\n",
    "    max_indices = max_values.idxmax()\n",
    "    \n",
    "    # Extract the day component from the DatetimeIndex\n",
    "    days = [x.index[i].day for i in max_indices]\n",
    "    return pd.Series(days, index=max_indices.index)\n",
    "\n",
    "def ts_rank(x: pd.Series, d: int):\n",
    "    # Note that pandas' rolling rank is deprecated; use expanding instead\n",
    "    return x.expanding(min_periods=d).rank()\n",
    "\n",
    "def min(x: pd.Series, d: int):\n",
    "    return ts_min(x, d)\n",
    "\n",
    "def max(x: pd.Series, d: int):\n",
    "    return ts_max(x, d)\n",
    "\n",
    "def sum(x: pd.Series, d: int):\n",
    "    return x.rolling(d).sum()\n",
    "\n",
    "def product(x: pd.Series, d: int):\n",
    "    # Be cautious with this function as it could lead to underflow/overflow issues\n",
    "    return x.rolling(d).apply(np.prod, raw=True)\n",
    "\n",
    "def stddev(x: pd.Series, d: int):\n",
    "    return x.rolling(d).std()\n",
    "\n",
    "\n",
    "    \n",
    "for length_ in bounds_list:\n",
    "\n",
    "    df['Ccv+str(length_)'] = df['Close'].rolling(window=length_).std() / df['Close'].rolling(window=length_).mean()\n",
    "    df['Hcv+str(length_)'] = df['High'].rolling(window=length_).std() / df['High'].rolling(window=length_).mean()\n",
    "    df['Lcv+str(length_)'] = df['Low'].rolling(window=length_).std() / df['Low'].rolling(window=length_).mean()\n",
    "    df['Vcv+str(length_)'] = df['Volume'].rolling(window=length_).std() / df['Volume'].rolling(window=length_).mean()\n",
    "    df['Wcv+str(length_)'] = df['VWAP_D'].rolling(window=length_).std() / df['VWAP_D'].rolling(window=length_).mean()\n",
    "\n",
    "    df['Crange_values'+str(length_)] = df['Close'].rolling(window=length_).max() - df['Close'].rolling(window=length_).min()\n",
    "    df['Hrange_values'+str(length_)] = df['High'].rolling(window=length_).max() - df['High'].rolling(window=length_).min()\n",
    "    df['Lrange_values'+str(length_)] = df['Low'].rolling(window=length_).max() - df['Low'].rolling(window=length_).min()\n",
    "    df['Vrange_values'+str(length_)] = df['Volume'].rolling(window=length_).max() - df['Volume'].rolling(window=length_).min()\n",
    "    df['Wrange_values'+str(length_)] = df['VWAP_D'].rolling(window=length_).max() - df['VWAP_D'].rolling(window=length_).min()\n",
    "\n",
    "    df['Cpercentile_90'+str(length_)] = df['Close'].rolling(window=length_).quantile(0.9)\n",
    "    df['Hpercentile_90'+str(length_)] = df['High'].rolling(window=length_).quantile(0.9)\n",
    "    df['Lpercentile_90'+str(length_)] = df['Low'].rolling(window=length_).quantile(0.9)\n",
    "    df['Vpercentile_90'+str(length_)] = df['Volume'].rolling(window=length_).quantile(0.9)\n",
    "    df['Wpercentile_90'+str(length_)] = df['VWAP_D'].rolling(window=length_).quantile(0.9)\n",
    "\n",
    "\n",
    "    df[f'Cautcorr{length_}'] = df['Close'].autocorr(lag=length_) \n",
    "    df[f'Hautcorr{length_}'] = df['High'].autocorr(lag=length_) \n",
    "    df[f'Lautcorr{length_}'] = df['Low'].autocorr(lag=length_) \n",
    "    df[f'Vautcorr{length_}'] = df['Volume'].autocorr(lag=length_) \n",
    "    df[f'Wautcorr{length_}'] = df['VWAP_D'].autocorr(lag=length_) \n",
    "\n",
    "\n",
    "    df[f'CVKMclst{length_}'] = kmeans.fit_predict(df[[f'Cautcorr{length_}', f'Vautcorr{length_}']])\n",
    "    df[f'CWKMclst{length_}'] = kmeans.fit_predict(df[[f'Cautcorr{length_}', f'Wautcorr{length_}']])\n",
    "\n",
    "\n",
    "    df[f'HVKMclst{length_}'] = kmeans.fit_predict(df[[f'Hautcorr{length_}', f'Vautcorr{length_}']])\n",
    "    df[f'HWKMclst{length_}'] = kmeans.fit_predict(df[[f'Hautcorr{length_}', f'Wautcorr{length_}']])\n",
    "\n",
    "    df[f'LVKMclst{length_}'] = kmeans.fit_predict(df[[f'Lautcorr{length_}', f'Vautcorr{length_}']])\n",
    "    df[f'LWKMclst{length_}'] = kmeans.fit_predict(df[[f'Lautcorr{length_}', f'Wautcorr{length_}']])\n",
    "\n",
    "\n",
    "    df['entropy'+ str(length_)] = ta.entropy(df['Close'], length_)\n",
    "    df['kurtosis'+ str(length_)] = ta.kurtosis(df['Close'], length_)\n",
    "    df['mad'+ str(length_)] = ta.mad(df['Volume'], length_)\n",
    "    df['quantile_50'+ str(length_)] = ta.quantile(df['Close'], q=0.5,  window=length_)  # For 50th percentile (median)\n",
    "\n",
    "\n",
    "\n",
    "    df['del_val'+ str(length_)] = delay(df['Close'], length_)\n",
    "\n",
    "    df['cov_val'+ str(length_)] = rolling_covariance(df['Close'], df['L_C'+str(length_)], length_)\n",
    "\n",
    " \n",
    "\n",
    "    df['amihud'] = (2*(df['High'] - df['Low']) - abs(df['Open'] - df['Close'])) / df['Volume']\n",
    "    df['rolling_std'+str(length_)] = df['Close'].rolling(window=length_).std()\n",
    "\n",
    "    df['min_val'+ str(length_)] = ts_min(df['Close'], length_)\n",
    "    # Other time-series operations using ts_operator\n",
    "    df['max_val'+ str(length_)] = ts_max(df['Close'], length_)\n",
    "\n",
    "    \n",
    "\n",
    "start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "for col in df.columns:\n",
    "    col_type = df[col].dtype\n",
    "\n",
    "    if col_type != object:\n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "        if str(col_type)[:3] == \"int\":\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "\n",
    "\n",
    "logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "end_mem = df.memory_usage().sum() / 1024**2\n",
    "logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a624d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:26.338957Z",
     "iopub.status.busy": "2024-02-11T01:06:26.338507Z",
     "iopub.status.idle": "2024-02-11T01:06:26.593357Z",
     "shell.execute_reply": "2024-02-11T01:06:26.592054Z"
    },
    "papermill": {
     "duration": 0.26478,
     "end_time": "2024-02-11T01:06:26.595585",
     "exception": false,
     "start_time": "2024-02-11T01:06:26.330805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: (1500, 363)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "df = df[slow_Upper:]\n",
    "df = df.dropna(axis=1,how='all')\n",
    "\n",
    "# Replace infinity and large values with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "# Now proceed with the imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Identify and drop columns with all the same value in-place\n",
    "df.drop(df.columns[df.nunique() == 1], axis=1, inplace=True)\n",
    "\n",
    "columns_to_drop = ['tarPer_C', 'tarPer_H', 'tarPer_L', 'tarPer_V', 'ts']\n",
    "\n",
    "yc = df['tarPer_C']\n",
    "yh = df['tarPer_H']\n",
    "yl = df['tarPer_L']\n",
    "yv = df['tarPer_V']\n",
    "\n",
    "df = df.drop(columns = columns_to_drop, axis=1)\n",
    "\n",
    "print(f'df: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7cc3c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:26.611717Z",
     "iopub.status.busy": "2024-02-11T01:06:26.611311Z",
     "iopub.status.idle": "2024-02-11T01:06:26.693845Z",
     "shell.execute_reply": "2024-02-11T01:06:26.692745Z"
    },
    "papermill": {
     "duration": 0.094297,
     "end_time": "2024-02-11T01:06:26.697386",
     "exception": false,
     "start_time": "2024-02-11T01:06:26.603089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2:(1500, 30)\n"
     ]
    }
   ],
   "source": [
    "foInd = df.columns.get_loc('divbull_l')\n",
    "df1 = df.iloc[:, foInd+1:]\n",
    "df0 = df.iloc[:, :foInd]\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with n columns where n > 30\n",
    "pca = PCA(n_components=30)\n",
    "\n",
    "# Fit and transform the data\n",
    "principal_components = pca.fit_transform(df1)\n",
    "\n",
    "# Create a new DataFrame with the first 30 principal components\n",
    "df2 = pd.DataFrame(principal_components, columns=[f'PC{i}' for i in range(1, 31)])\n",
    "#df = pd.comcat([df0,df2],axis = 1)\n",
    "print(f'df2:{df2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44d6338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:26.729113Z",
     "iopub.status.busy": "2024-02-11T01:06:26.728515Z",
     "iopub.status.idle": "2024-02-11T01:06:27.562817Z",
     "shell.execute_reply": "2024-02-11T01:06:27.561187Z"
    },
    "papermill": {
     "duration": 0.852955,
     "end_time": "2024-02-11T01:06:27.565400",
     "exception": false,
     "start_time": "2024-02-11T01:06:26.712445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfgp\"(1500, 10)\n"
     ]
    }
   ],
   "source": [
    "from gplearn.genetic import SymbolicTransformer\n",
    "import numpy as np\n",
    "\n",
    "function_set = ['add', 'sub', 'mul', 'div',\n",
    "                'sqrt', 'log', 'abs', 'neg', 'inv',\n",
    "                'max', 'min', 'sin', 'cos', 'tan']\n",
    "\n",
    "n_components_=10\n",
    "\n",
    "gp = SymbolicTransformer( population_size=200, hall_of_fame=80, n_components=n_components_, generations=3, tournament_size=1, stopping_criteria=1.0, const_range=(-1.0, 1.0), init_depth=(2, 6), init_method='half and half', function_set=function_set, metric='pearson', parsimony_coefficient=0.001, p_crossover=0.9, p_subtree_mutation=0.01, p_hoist_mutation=0.01, p_point_mutation=0.01, p_point_replace=0.05, max_samples=1.0, feature_names=None, warm_start=False, low_memory=False, n_jobs=1, verbose=0, random_state=None)\n",
    " \n",
    "\n",
    "# Fit the model\n",
    "gp.fit(df, yc)\n",
    "dfgp = pd.DataFrame(data = gp.transform(df))\n",
    "\n",
    "print(f'dfgp\"{dfgp.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4448dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:27.581407Z",
     "iopub.status.busy": "2024-02-11T01:06:27.580995Z",
     "iopub.status.idle": "2024-02-11T01:06:27.589364Z",
     "shell.execute_reply": "2024-02-11T01:06:27.588207Z"
    },
    "papermill": {
     "duration": 0.019004,
     "end_time": "2024-02-11T01:06:27.591591",
     "exception": false,
     "start_time": "2024-02-11T01:06:27.572587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:(1500, 59)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df0, df2, dfgp], axis = 1)\n",
    "print(f'df:{df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7168f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:27.607426Z",
     "iopub.status.busy": "2024-02-11T01:06:27.607063Z",
     "iopub.status.idle": "2024-02-11T01:06:27.614456Z",
     "shell.execute_reply": "2024-02-11T01:06:27.613582Z"
    },
    "papermill": {
     "duration": 0.02014,
     "end_time": "2024-02-11T01:06:27.618895",
     "exception": false,
     "start_time": "2024-02-11T01:06:27.598755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:(1499, 59)\n"
     ]
    }
   ],
   "source": [
    "X = df[:-1]   \n",
    "pyc = yc[:-1]\n",
    "pyh = yh[:-1]\n",
    "pyl = yl[:-1]\n",
    "pyv = yv[:-1]\n",
    "         \n",
    "news = df[-1:]\n",
    "nyc = yc[-1:]\n",
    "nyh = yh[-1:]\n",
    "nyl = yl[-1:]\n",
    "nyv = yv[-1:]\n",
    "print(f'X:{X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91513dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:27.644053Z",
     "iopub.status.busy": "2024-02-11T01:06:27.643554Z",
     "iopub.status.idle": "2024-02-11T01:06:27.660242Z",
     "shell.execute_reply": "2024-02-11T01:06:27.659034Z"
    },
    "papermill": {
     "duration": 0.028271,
     "end_time": "2024-02-11T01:06:27.662585",
     "exception": false,
     "start_time": "2024-02-11T01:06:27.634314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport lightgbm as lgb  \\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\\nfrom lightgbm import LGBMRegressor, cv\\nfrom sklearn.model_selection import train_test_split\\nimport lightgbm as lgb\\nfrom sklearn.metrics import mean_squared_error\\n\\n\\n# Define searched space\\nhyper_space = {'objective': 'regression',\\n               'metric':'rmse',\\n               'boosting':'gbdt', 'device':'gpu',#'gpu_device_id': 0,\\n               #'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),\\n               'max_depth':  hp.choice('max_depth', list(range(6, 18, 2))),\\n               'num_leaves': hp.choice('num_leaves', list(range(20, 180, 20))),\\n               'subsample': hp.choice('subsample', [.7, .8, .9, 1]),\\n               'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1),\\n               'learning_rate': hp.uniform('learning_rate', 0.03, 0.12),\\n               #'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6]),\\n               #'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]),               \\n               'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100])\\n              }\\n\\n\\n# Assuming df is your DataFrame with features in all columns except the last one (target)\\n\\ny = pyc\\n\\n# Split the data into training and testing sets (80% for training, 20% for testing)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\nparams = {\\n    'boosting_type': 'gbdt',\\n    # Add more params as needed\\n    'verbosity': -1,\\n    'early_stopping_round': 10\\n}\\n\\ndef evaluate_metric(params):\\n\\n    # Convert data to DMatrix format required by LightGBM\\n    dtrain = lgb.Dataset(X_train, label=y_train)\\n    dtest = lgb.Dataset(X_test, label=y_test)\\n\\n    lgbm_reg = lgb.train(params, dtrain, 2000, valid_sets = [dtest])\\n                        \\n    pred_lgb = lgbm_reg.predict(X_test, num_iteration=lgbm_reg.best_iteration)\\n\\n    score_uni = np.sqrt(mean_squared_error(pred_lgb, y_test))\\n    print(f'Score Validation : {score_uni}')\\n    return score_uni\\n\\n# Seting the number of Evals\\nMAX_EVALS= 15\\n\\n# Fit Tree Parzen Estimator\\nbest_vals = fmin(evaluate_metric, \\n                 space=hyper_space,\\n                 verbose=--1,\\n                 algo=tpe.suggest, \\n                 max_evals=MAX_EVALS)\\n\\n# Print best parameters\\nbest_params = space_eval(hyper_space, best_vals)\\n\\nprint(best_params)\\n\\nparams = best_params\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import lightgbm as lgb  \n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from lightgbm import LGBMRegressor, cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Define searched space\n",
    "hyper_space = {'objective': 'regression',\n",
    "               'metric':'rmse',\n",
    "               'boosting':'gbdt', 'device':'gpu',#'gpu_device_id': 0,\n",
    "               #'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),\n",
    "               'max_depth':  hp.choice('max_depth', list(range(6, 18, 2))),\n",
    "               'num_leaves': hp.choice('num_leaves', list(range(20, 180, 20))),\n",
    "               'subsample': hp.choice('subsample', [.7, .8, .9, 1]),\n",
    "               'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1),\n",
    "               'learning_rate': hp.uniform('learning_rate', 0.03, 0.12),\n",
    "               #'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6]),\n",
    "               #'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]),               \n",
    "               'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100])\n",
    "              }\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame with features in all columns except the last one (target)\n",
    "\n",
    "y = pyc\n",
    "\n",
    "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    # Add more params as needed\n",
    "    'verbosity': -1,\n",
    "    'early_stopping_round': 10\n",
    "}\n",
    "\n",
    "def evaluate_metric(params):\n",
    "\n",
    "    # Convert data to DMatrix format required by LightGBM\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dtest = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "    lgbm_reg = lgb.train(params, dtrain, 2000, valid_sets = [dtest])\n",
    "                        \n",
    "    pred_lgb = lgbm_reg.predict(X_test, num_iteration=lgbm_reg.best_iteration)\n",
    "\n",
    "    score_uni = np.sqrt(mean_squared_error(pred_lgb, y_test))\n",
    "    print(f'Score Validation : {score_uni}')\n",
    "    return score_uni\n",
    "\n",
    "# Seting the number of Evals\n",
    "MAX_EVALS= 15\n",
    "\n",
    "# Fit Tree Parzen Estimator\n",
    "best_vals = fmin(evaluate_metric, \n",
    "                 space=hyper_space,\n",
    "                 verbose=--1,\n",
    "                 algo=tpe.suggest, \n",
    "                 max_evals=MAX_EVALS)\n",
    "\n",
    "# Print best parameters\n",
    "best_params = space_eval(hyper_space, best_vals)\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "params = best_params\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57171afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:27.680704Z",
     "iopub.status.busy": "2024-02-11T01:06:27.679620Z",
     "iopub.status.idle": "2024-02-11T01:06:40.516711Z",
     "shell.execute_reply": "2024-02-11T01:06:40.515751Z"
    },
    "papermill": {
     "duration": 12.848943,
     "end_time": "2024-02-11T01:06:40.519171",
     "exception": false,
     "start_time": "2024-02-11T01:06:27.670228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 14574\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499, number of used features: 59\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 59 dense feature groups (0.09 MB) transferred to GPU in 0.001762 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.001895\n",
      "Prediction for the new data:  -0.0012964125245337283\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 14574\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499, number of used features: 59\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 59 dense feature groups (0.09 MB) transferred to GPU in 0.001422 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.024958\n",
      "Prediction for the new data:  0.010212617654168022\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 14574\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499, number of used features: 59\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 59 dense feature groups (0.09 MB) transferred to GPU in 0.001506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.025470\n",
      "Prediction for the new data:  -0.011982146588984777\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 14574\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499, number of used features: 59\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 59 dense feature groups (0.09 MB) transferred to GPU in 0.001410 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.142934\n",
      "Prediction for the new data:  0.3229242282133509\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device_type = 'gpu'\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',  # 回归任务\n",
    "    'metric': 'l2',  # 均方误差作为评估指标   'device':'gpu'\n",
    "    'device': device_type,\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "}\n",
    "\n",
    "# 创建Dataset\n",
    "train_data = lgb.Dataset(X, label=pyc)\n",
    "# 训练模型\n",
    "model = lgb.train(params, train_set=train_data, num_boost_round=100)\n",
    "\n",
    "# 预测单行数据\n",
    "#new_data = gpNews.reshape(1, -1)  # 将测试数据重塑为1行\n",
    "Cprd = model.predict(news)\n",
    "print(\"Prediction for the new data: \", Cprd[0])\n",
    "\n",
    "# 创建Dataset\n",
    "train_data = lgb.Dataset(X, label=pyh)\n",
    "# 训练模型\n",
    "model = lgb.train(params, train_set=train_data, num_boost_round=100)\n",
    "# 预测单行数据\n",
    "#new_data = gpNews.reshape(1, -1)  # 将测试数据重塑为1行\n",
    "Hprd = model.predict(news)\n",
    "print(\"Prediction for the new data: \", Hprd[0])\n",
    "\n",
    "# 创建Dataset\n",
    "train_data = lgb.Dataset(X, label=pyl)\n",
    "# 训练模型\n",
    "model = lgb.train(params, train_set=train_data, num_boost_round=100)\n",
    "# 预测单行数据\n",
    "#new_data = gpNews.reshape(1, -1)  # 将测试数据重塑为1行\n",
    "Lprd = model.predict(news)\n",
    "print(\"Prediction for the new data: \", Lprd[0])\n",
    "\n",
    "# 创建Dataset\n",
    "train_data = lgb.Dataset(X, label=pyv)\n",
    "# 训练模型\n",
    "model = lgb.train(params, train_set=train_data, num_boost_round=100)\n",
    "# 预测单行数据\n",
    "#new_data = gpNews.reshape(1, -1)  # 将测试数据重塑为1行\n",
    "Vprd = model.predict(news)\n",
    "print(\"Prediction for the new data: \", Vprd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e088c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:40.543997Z",
     "iopub.status.busy": "2024-02-11T01:06:40.543643Z",
     "iopub.status.idle": "2024-02-11T01:06:40.549294Z",
     "shell.execute_reply": "2024-02-11T01:06:40.548252Z"
    },
    "papermill": {
     "duration": 0.019447,
     "end_time": "2024-02-11T01:06:40.551948",
     "exception": false,
     "start_time": "2024-02-11T01:06:40.532501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': -0.0012964125245337283, 'h': 0.010212617654168022, 'l': -0.011982146588984777, 'v': 0.3229242282133509}\n"
     ]
    }
   ],
   "source": [
    "pdic = {'c':Cprd[0], 'h': Hprd[0], 'l': Lprd[0], 'v': Vprd[0]}\n",
    "print(pdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efa18154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T01:06:40.573611Z",
     "iopub.status.busy": "2024-02-11T01:06:40.573234Z",
     "iopub.status.idle": "2024-02-11T01:06:40.584345Z",
     "shell.execute_reply": "2024-02-11T01:06:40.583253Z"
    },
    "papermill": {
     "duration": 0.024353,
     "end_time": "2024-02-11T01:06:40.586497",
     "exception": false,
     "start_time": "2024-02-11T01:06:40.562144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b1': array([47289.80281069]), 'b2': array([47391.87294248]), 'b3': array([47493.94307426]), 'a1': array([48137.8203629]), 'a2': array([48027.88610663]), 'a3': array([47917.95185036]), 'upstop': 48271.87849647264, 'lowstop': 47164.13881257063}\n"
     ]
    }
   ],
   "source": [
    "pc = news['Close'].values*(Cprd[0]+1)\n",
    "ph = news['Close'].values*(Hprd[0]+1)\n",
    "pl = news['Close'].values*(Lprd[0]+1)\n",
    "pv = news['Volume'].values*(Vprd[0]+1)\n",
    "\n",
    "pmin = np. min([pc, ph, pl])\n",
    "pmax = np. max([pc, ph, pl])\n",
    "lowste=(pc-pmin)/5\n",
    "upste=(pmax-pc)/5\n",
    "\n",
    "b1=pmin+lowste\n",
    "b2=pmin+2*lowste\n",
    "b3=pmin+3*lowste\n",
    "\n",
    "a1=pmax-upste\n",
    "a2=pmax - 2* upste\n",
    "a3= pmax - 3 * upste\n",
    "\n",
    "upstop = pmax * 1.0005\n",
    "lowstop = pmin * 0.9995\n",
    "odDic = {'b1':b1  , 'b2':b2  , 'b3':b3  , 'a1': a1 , 'a2':a2  , 'a3':a3  , 'upstop': upstop , 'lowstop':lowstop} \n",
    "print(odDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf8602",
   "metadata": {
    "papermill": {
     "duration": 0.010845,
     "end_time": "2024-02-11T01:06:40.608807",
     "exception": false,
     "start_time": "2024-02-11T01:06:40.597962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.164626,
   "end_time": "2024-02-11T01:06:41.539293",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-11T01:05:38.374667",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
